{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io  # load the matrix file\n",
    "import pickle as cp\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten\n",
    "from keras.layers.core import Permute, Reshape\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from sliding_window import sliding_window\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Opportunity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      " ..from file data/gestures_wearable_ambient.data\n",
      " ..reading instances: X_train (557963, 210), X_test (118750, 210)\n",
      " ..reading instances: y_train (557963,), y_test (118750,)\n",
      "Successfully\n",
      "------------------------------------------------------------\n",
      "Data normalizing...\n",
      "Data normalized...\n",
      "------------------------------------------------------------\n",
      "Spliting the data into F & A... \n",
      "Successfully\n",
      "------------------------------------------------------------\n",
      "17 actual activities and 1 NaN activity,\n",
      "Total 18 Classes\n",
      "F_train shape is  (557963, 113)\n",
      "F_test shape is  (118750, 113)\n",
      "A_train shape is  (557963, 97)\n",
      "A_test shape is  (118750, 97)\n",
      "y_train shape is  (557963,)\n",
      "y_test shape is  (118750,)\n",
      "------------------------------------------------------------\n",
      "Opperating sliding window...\n",
      " ..after sliding window (training):\n",
      " inputs : (wearable sensor data) F_train0(46495, 24, 113) , (ambient sensor data) A_train0(46495, 24, 97), targets(label) y_train0 (46495,)\n",
      " ..after sliding window (testing):\n",
      " inputs : (wearable sensor data) F_test0(9894, 24, 113) , (ambient sensor data) A_test0(9894, 24, 97), targets(label) y_test0 (9894,)\n",
      "Successfully\n",
      "------------------------------------------------------------\n",
      "Changing targets shape... (One-Hot Encoder for cross-entropy loss ) \n",
      "Before transforming: y_train0 shape (46495,), y_test0 shape (9894,) \n",
      "After transforming: y_train1 shape (46495, 18),y_test1 shape (9894, 18)\n",
      "Successfully\n",
      "------------------------------------------------------------\n",
      "X_train shape (46495, 24, 113, 1)\n",
      "X_valid shape (9894, 24, 113, 1)\n",
      "y_train.shape(46495, 18)\n",
      "y_train.shape(46495, 18)\n",
      "Reshape the X_train and X_valid for CNN input\n",
      "X_train_cnn shape (46495, 113, 24, 1)\n",
      "X_valid_cnn shape (9894, 113, 24, 1)\n",
      "X_train.shape(46495, 18)\n",
      "y_train.shape(46495, 18)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "    Function to load dataset\n",
    "\n",
    "    Argument:\n",
    "        filename : the path of the preprocessed dataset\n",
    "\n",
    "    Return:\n",
    "\n",
    "        X_train\n",
    "        y_train\n",
    "        X_test\n",
    "        y_test\n",
    "\n",
    "    Notice:\n",
    "        Need use `pickle(python3)` or`cpicckle(python2)` module to load and read dataset.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "    data = cp.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "\n",
    "    print(\" ..from file {}\".format(filename))\n",
    "    print(\" ..reading instances: X_train {0}, X_test {1}\".format(\n",
    "        X_train.shape, X_test.shape))\n",
    "    print(\" ..reading instances: y_train {0}, y_test {1}\".format(\n",
    "        y_train.shape, y_test.shape))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # The targets are casted to int8 for GPU compatibility.\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "# X_train, y_train, X_test, y_test = load_dataset('DeepConvLSTM/data/gestures_wearable_ambient.data')\n",
    "X_train, y_train, X_test, y_test = load_dataset('data/gestures_wearable_ambient.data')\n",
    "# /Users/chentailin/sharedfolder/Human-Activity-Recognition-CodeHub/opportinity-dataset/DeepConvLSTM/data/gestures_wearable_ambient.data\n",
    "\n",
    "X_train_origin = X_train.copy()\n",
    "y_train_origin = y_train.copy()\n",
    "X_test_origin, y_test_origin = X_test.copy(), y_test.copy()\n",
    "\n",
    "print(\"Successfully\")\n",
    "\n",
    "print(\"---\" * 20)\n",
    "\n",
    "# Normalization the Raw data\n",
    "\n",
    "\n",
    "def normalization_x_y(x, y):\n",
    "    \"\"\"\n",
    "    Stack x and y, then do normalization\n",
    "    Then return DataFrame x and y\n",
    "    \"\"\"\n",
    "    print(\"Data normalizing...\")\n",
    "    data = np.vstack((x, y))\n",
    "    df = pd.DataFrame(data)\n",
    "    df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    data_x = df.iloc[:557963, ]\n",
    "    data_y = df.iloc[557963:, ]\n",
    "\n",
    "    data_x = np.array(data_x)\n",
    "    data_y = np.array(data_y)\n",
    "\n",
    "    print(\"Data normalized...\")\n",
    "    return data_x, data_y\n",
    "\n",
    "\n",
    "# Normalization the whole dataset\n",
    "X_train_nor, X_test_nor = normalization_x_y(X_train, X_test)\n",
    "\n",
    "\n",
    "# split the wearable sensor data and ambient sensor data\n",
    "print(\"---\" * 20)\n",
    "print(\"Spliting the data into F & A... \")\n",
    "X_train_F = X_train_nor[:, :113]\n",
    "X_train_A = X_train_nor[:, 113:]\n",
    "\n",
    "X_test_F = X_test_nor[:, :113]\n",
    "X_test_A = X_test_nor[:, 113:]\n",
    "\n",
    "# More clearly split\n",
    "F_train = X_train_F\n",
    "F_test = X_test_F\n",
    "\n",
    "A_train = X_train_A\n",
    "A_test = X_test_A\n",
    "print(\"Successfully\")\n",
    "\n",
    "print(\"---\" * 20)\n",
    "print(\"{0} actual activities and 1 NaN activity,\\nTotal 18 Classes\".format(y_train.max()))\n",
    "\n",
    "# Opportunity num_classes = 18\n",
    "num_classes = 18\n",
    "\n",
    "print(\"F_train shape is \", F_train.shape)\n",
    "print(\"F_test shape is \", F_test.shape)\n",
    "\n",
    "print(\"A_train shape is \", A_train.shape)\n",
    "print(\"A_test shape is \", A_test.shape)\n",
    "\n",
    "\n",
    "print(\"y_train shape is \", y_train.shape)\n",
    "print(\"y_test shape is \", y_test.shape)\n",
    "\n",
    "print(\"---\" * 20)\n",
    "\n",
    "print(\"Opperating sliding window...\")\n",
    "\"\"\"\n",
    "Define Sliding Window\n",
    "\"\"\"\n",
    "SLIDING_WINDOW_LENGTH = 24\n",
    "SLIDING_WINDOW_STEP = 12\n",
    "\n",
    "# from sliding_window import sliding_window\n",
    "# ?sliding_window\n",
    "\n",
    "# assert NB_SENSOR_CHANNELS == X_train.shape[1]\n",
    "\n",
    "\n",
    "def opp_sliding_window(data_x, data_a, data_y, ws, ss):\n",
    "    \"\"\"\n",
    "   Parameters: (F_train,A_train,y_train,window_size,step_size)\n",
    "\n",
    "    \"\"\"\n",
    "    data_x = sliding_window(a=data_x, ws=(\n",
    "        ws, data_x.shape[1]), ss=(ss, 1), flatten=False)\n",
    "#     data_x = sliding_window(data_x,ws,ss)\n",
    "    data_a = sliding_window(a=data_a, ws=(\n",
    "        ws, data_a.shape[1]), ss=(ss, 1), flatten=False)\n",
    "    data_y = np.asarray([[i[-1]]\n",
    "                         for i in sliding_window(data_y, ws, ss, flatten=False)])\n",
    "    return data_x.reshape(-1, ws, data_x.shape[3]).astype(np.float32), data_a.reshape(-1, ws, data_a.shape[3]).astype(np.float32), data_y.reshape(-1).astype(np.uint8)\n",
    "#     return data_x.astype(np.float32), data_y.flatten().astype(np.uint8)\n",
    "\n",
    "\n",
    "F_train0, A_train0, y_train0 = opp_sliding_window(\n",
    "    F_train, A_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "print(\" ..after sliding window (training):\\n inputs : (wearable sensor data) F_train0{0} , (ambient sensor data) A_train0{1}, targets(label) y_train0 {2}\".format(\n",
    "    F_train0.shape, A_train0.shape, y_train0.shape))\n",
    "\n",
    "F_test0, A_test0, y_test0 = opp_sliding_window(\n",
    "    F_test, A_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "print(\" ..after sliding window (testing):\\n inputs : (wearable sensor data) F_test0{0} , (ambient sensor data) A_test0{1}, targets(label) y_test0 {2}\".format(\n",
    "    F_test0.shape, A_test0.shape, y_test0.shape))\n",
    "\n",
    "print(\"Successfully\")\n",
    "print(\"---\" * 20)\n",
    "\n",
    "\n",
    "print(\"Changing targets shape... (One-Hot Encoder for cross-entropy loss ) \")\n",
    "y_train1 = to_categorical(y=y_train0, num_classes=num_classes)\n",
    "y_test1 = to_categorical(y=y_test0, num_classes=num_classes)\n",
    "print(\"Before transforming: y_train0 shape {0}, y_test0 shape {2} \\nAfter transforming: y_train1 shape {1},y_test1 shape {3}\".format(\n",
    "    y_train0.shape, y_train1.shape, y_test0.shape, y_test1.shape))\n",
    "\n",
    "print(\"Successfully\")\n",
    "\n",
    "print(\"---\" * 20)\n",
    "X_train = np.reshape(\n",
    "    F_train0, (F_train0.shape[0], F_train0.shape[1], F_train0.shape[2], 1))  # reshape末尾增加1维\n",
    "X_valid = np.reshape(\n",
    "    F_test0, (F_test0.shape[0], F_test0.shape[1], F_test0.shape[2], 1))\n",
    "y_train = y_train1\n",
    "y_valid = y_test1\n",
    "\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_valid shape\", X_valid.shape)\n",
    "print('y_train.shape' + str(y_train.shape))\n",
    "print('y_train.shape' + str(y_train.shape))\n",
    "\n",
    "print(\"Reshape the X_train and X_valid for CNN input\")\n",
    "X_train_cnn = np.swapaxes(X_train, 1, 2)  # 就是将第三个维度和第二个维度交换\n",
    "X_valid_cnn = np.swapaxes(X_valid, 1, 2)\n",
    "\n",
    "print(\"X_train_cnn shape\", X_train_cnn.shape)\n",
    "print(\"X_valid_cnn shape\", X_valid_cnn.shape)\n",
    "print('X_train.shape' + str(y_train.shape))\n",
    "print('y_train.shape' + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Now, all the data are prepared to be input into the Deep Model\n",
    "\n",
    "- ` F_train0 ` ------> ` y_train1 ` <------` A_train0 `\n",
    "\n",
    "- ` F_test0 ` ------>   ` y_test1 ` <------` A_test0 `\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we noly choose **F** as the feature and **y** as the label\n",
    "- Let \n",
    "        X_train =F_train0\n",
    "        X_test = F_test0\n",
    "        y_train = y_train1\n",
    "        y_test = y_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9894, 24, 113)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_train0.shape\n",
    "F_test0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (46495, 24, 113, 1)\n",
      "X_valid shape (9894, 24, 113, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(F_train0,(F_train0.shape[0],F_train0.shape[1],F_train0.shape[2],1)) # reshape末尾增加1维\n",
    "X_valid = np.reshape(F_test0,(F_test0.shape[0],F_test0.shape[1],F_test0.shape[2],1))\n",
    "y_train = y_train1\n",
    "y_valid = y_test1\n",
    "\n",
    "\n",
    "print(\"X_train shape\" , X_train.shape)\n",
    "print(\"X_valid shape\" , X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape(46495, 24, 113, 1)\n",
      "Y_train.shape(46495, 18)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape' + str(X_train.shape))\n",
    "print('Y_train.shape' + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_basic(x, W, bias):\n",
    "    \"\"\"\n",
    "    x: input data\n",
    "    W: weights\n",
    "    bias: bias\n",
    "    \"\"\"\n",
    "    conv = tf.nn.conv2d(x,W, strides=[1,1,1,1], padding = 'SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "\n",
    "def maxpool_1x2(x):  # a 2*2 size of the window and 2*2 movement of the window step no overlapping\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 2, 1], strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob):\n",
    "    return tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "def permute(x, permute_list):\n",
    "    return tf.transpose(x, perm=permute_list)\n",
    "\n",
    "\n",
    "def mini_batch(x, y, batch_size):\n",
    "    mini_batches = []\n",
    "    num_bathches = int(np.floor(x.shape[0] / batch_size))\n",
    "    for batch_index in range(num_bathches):\n",
    "        mini_batch_x = x[batch_index *\n",
    "                         batch_size:((batch_index * batch_size) + batch_size), :, :, :]\n",
    "        mini_batch_y = y[batch_index *\n",
    "                         batch_size:((batch_index * batch_size) + batch_size), :]\n",
    "        mini_batch = (mini_batch_x, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def create_placeholders(dim, win_len, n_C0, num_classes):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dim :           scalar, dimension of the input ()\n",
    "    win_len :       scalar, window_size of the input()\n",
    "    n_C0 :          scalar, number of channels of the input(commonly 1,time-series data)\n",
    "    num_classes :   sclar, number of classes (num_classes(18))\n",
    "\n",
    "    Return:\n",
    "    x :  placeholder for the data input, of shape[None, win_length,dim,n_C0] and type = 'float'\n",
    "    y :  placeholder for the input labels, of shape[None, num_classes] and type = 'float'\n",
    "    X_train shape is (46495,24,113,1)-->(num_samples,window_length,feature_dims,num_channels)\n",
    "\n",
    "    Now the x is (46495,113,24,1)\n",
    "    \"\"\"\n",
    "    x = tf.placeholder('float32', shape=[None, dim, win_len, n_C0])\n",
    "    y = tf.placeholder('float32', shape=[None, num_classes])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initialize parameters to be used in this model,  dictionary-like!\n",
    "    This method is to create variables basically in our model\n",
    "    did't actually create the neural_network_model\n",
    "    Process:\n",
    "    Input --> Conv2D(num_feat_map,kernel_size=(1,5),padding=same) --> MaxPooling2D(pool_size=(1,2),解释[1])-->\n",
    "    Dropout-->Conv2D(同上                                        ) --> MaxPooling2D (                     )--> Dropout-->Conv2D_Output\n",
    "          --> LSTM_0 -> LSTM_1 -> Dense/Output(num_classes)\n",
    "\n",
    "    解释[1]: pool_size为(1,2) 则原始数据的structure = (dim,win_length，channel） 经过pool_size之后dim不变，win_length减半，和channel无关（不对channel进行操作）若pool_size=（2，1），则dim减半（floor除 地板除结果）， win_length不变\n",
    "    \"\"\"\n",
    "\n",
    "def convolutional_lstm_neural_network(x):\n",
    "\n",
    "        # 5*5 parameters 1 input and 32 feature maps\n",
    "    cnn_weights = {'W_conv1': tf.Variable(tf.random_normal([1, 5, 1, 16])), 'W_conv2': tf.Variable(tf.random_normal([1, 5, 16, 16]))\n",
    "                   }\n",
    "    # biases just the number of the output\n",
    "    cnn_biases = {'b_conv1': tf.Variable(tf.random_normal([16])),  # 5*5 parameters 1 input and 32 feature maps\n",
    "                  'b_conv2': tf.Variable(tf.random_normal([16]))\n",
    "                  # 'b_fc': tf.Variable(tf.random_normal([1024])),\n",
    "                  # 'out': tf.Variable(tf.random_normal([n_classes])),\n",
    "                  }\n",
    "\n",
    "    lstm_weights = {'weights': tf.Variable(tf.random_normal(\n",
    "        [rnn_size, n_classes]))}  # its a parameters dictionary\n",
    "\n",
    "    lstm_biases = {'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    # We get all the parameters then define the model\n",
    "\n",
    "    \"\"\"\n",
    "    Define the data flow chart\n",
    "    \"\"\"\n",
    "    conv1 = conv2d_basic(x, cnn_weights['W_conv1'], cnn_biases['b_conv1']) \n",
    "    conv1 = relu(conv1)\n",
    "    conv1 = maxpool_1x2(conv1)\n",
    "    conv1 = dropout(conv1, keep_prob=keep_prob)\n",
    "\n",
    "    conv2 = conv2d(conv1, cnn_weights['W_conv2']) + cnn_biases['b_conv2']\n",
    "    conv2 = relu(conv2)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    conv2 = dropout(conv2, keep_prob)\n",
    "\n",
    "    # change the output shape (None, 113, 6, 16) to (None, 6, 113, 16)\n",
    "    reshape_1 = permute(conv2, [0, 2, 1, 3])\n",
    "    reshape_2 = tf.reshape(reshape_1, shape=[-1, 6, 113 * 16])\n",
    "\n",
    "    inputs = tf.unstack(reshape_2, 6, 1)\n",
    "\n",
    "    lstm_cell = rnn.BasicLSTMCell(rnn_size)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "    output = tf.matmul(\n",
    "        outputs[-1], lstm_weights['weights']) + lstm_biases['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_neural_network(X_train, y_train, X_valid, y_valid, learning_rate=0.001, num_epochs=6, batch_size=128, print_cost=True):\n",
    "\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    # initialize the placehodler parameters\n",
    "    (m, dim, win_len, n_C0) = X_train.shape\n",
    "    num_classes = y_train.shape[1]\n",
    "\n",
    "    # Create placeholders for X,y and keep_prob\n",
    "    x = tf.placeholder(dtype=tf.float32,shape=[None, dim, win_len, n_C0])\n",
    "    y = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])\n",
    "#     keep_prob = tf.placeholder(tf.float32)\n",
    "#     x, y = create_placeholders(dim, win_len, n_C0, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    # initialize parameters  \n",
    "    prediction = convolutional_lstm_neural_network(x) # prediction\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=prediction, labels=y)) # loss\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss) \n",
    "    \n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "#     writer = tf.summary.FileWriter('./graphs',tf.get_default_graph())\n",
    "    \n",
    "    \n",
    "    # start the session   \n",
    "    with tf.Session() as sess:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            minibatches = mini_batch(X_train, y_train, batch_size)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_x, minibatch_y) = minibatch\n",
    "\n",
    "                _, c = sess.run(\n",
    "                    [optimizer, loss], feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of',\n",
    "                  num_epochs, 'loss', epoch_loss)\n",
    "\n",
    "        print('Accuracy:', accuracy.eval({x: X_valid, y: y_valid}))\n",
    "\n",
    "#         writer.close()\n",
    "        print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-817a96ebd270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrnn_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m  \u001b[0;31m# num_hidden_lstm units number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "# keep_prob\n",
    "\n",
    "n_classes = 18\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "win_length = 24  # 2. imgaes are 28*28 you need to go into sequence. So 28 chunks of 28 pixels\n",
    "dim = 113\n",
    "\n",
    "rnn_size = 64  # num_hidden_lstm units number\n",
    "\n",
    "train_neural_network(X_train_cnn, y_train, X_valid_cnn, y_valid,num_epochs=num_epochs,learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 6 loss 603.9810857772827\n",
      "Epoch 1 completed out of 6 loss 533.0179053246975\n",
      "Epoch 2 completed out of 6 loss 522.526864439249\n",
      "Epoch 3 completed out of 6 loss 521.1721375882626\n",
      "Epoch 4 completed out of 6 loss 521.1198141276836\n",
      "Epoch 5 completed out of 6 loss 521.1422216296196\n",
      "Accuracy: 0.8325248\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(X_train_cnn, y_train, X_valid_cnn, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
