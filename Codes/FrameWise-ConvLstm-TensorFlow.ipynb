{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-78ffe68abce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msliding_window\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msliding_window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TF_CPP_MIN_LOG_LEVEL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io  # load the matrix file\n",
    "import pickle as cp\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten\n",
    "from keras.layers.core import Permute, Reshape\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from utils.sliding_window import sliding_window\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Opportunity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading data...\n",
      " Data already normalized...\n",
      " ..from file /Users/chentailin/Dropbox/Dataset/Opportunity/wearable_113_ambient_40_gestures_new.data\n",
      " ..reading instances: X_train (557963, 153), X_test (118750, 153)\n",
      " ..reading instances: y_train (557963,), y_test (118750,)\n",
      "--- Successfully\n",
      "============================================================\n",
      "--- Spliting the data into Wearable(F) & Ambient(A)... \n",
      "--- Successfully\n",
      "============================================================\n",
      "17 actual activities and 1 NaN activity,\n",
      "Total 18 Classes\n",
      "F_train shape is  (557963, 113)\n",
      "F_test shape is  (118750, 113)\n",
      "A_train shape is  (557963, 40)\n",
      "A_test shape is  (118750, 40)\n",
      "y_train shape is  (557963,)\n",
      "y_test shape is  (118750,)\n",
      "============================================================\n",
      "--- Changing targets shape... (One-Hot Encoder for cross-entropy loss ) \n",
      "Before transforming: y_train shape (557963,), y_test shape (118750,) \n",
      "After transforming: y_train_1hot shape (557963, 18),y_test_1hot shape (118750, 18)\n",
      "--- Successfully\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "    Function to load dataset\n",
    "\n",
    "    Argument:\n",
    "        filename : the path of the preprocessed dataset\n",
    "\n",
    "    Return:\n",
    "\n",
    "        X_train\n",
    "        y_train\n",
    "        X_test\n",
    "        y_test\n",
    "\n",
    "    Notice:\n",
    "        Need use `pickle(python3)` or`cpicckle(python2)` module to load and read dataset.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "    data = cp.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "\n",
    "    print(\" ..from file {}\".format(filename))\n",
    "    print(\" ..reading instances: X_train {0}, X_test {1}\".format(\n",
    "        X_train.shape, X_test.shape))\n",
    "    print(\" ..reading instances: y_train {0}, y_test {1}\".format(\n",
    "        y_train.shape, y_test.shape))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # The targets are casted to int8 for GPU compatibility.\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "print(\"--- Loading data...\\n\",\"Data already normalized...\")\n",
    "\n",
    "# X_train, y_train, X_test, y_test = load_dataset('C:/Users/41762/Dropbox/Dataset/Opportunity/wearable_113_ambient_40_gestures_new.data')\n",
    "X_train, y_train, X_test, y_test = load_dataset('/Users/chentailin/Dropbox/Dataset/Opportunity/wearable_113_ambient_40_gestures_new.data')\n",
    "\n",
    "print(\"--- Successfully\")\n",
    "\n",
    "\n",
    "# split the wearable sensor data and ambient sensor data\n",
    "print(\"===\" * 20)\n",
    "print(\"--- Spliting the data into Wearable(F) & Ambient(A)... \")\n",
    "F_train_F = X_train[:, :113]\n",
    "A_train_A = X_train[:, 113:]\n",
    "\n",
    "F_test = X_test[:, :113]\n",
    "A_test = X_test[:, 113:]\n",
    "print(\"--- Successfully\")\n",
    "print(\"===\" * 20)\n",
    "\n",
    "print(\"{0} actual activities and 1 NaN activity,\\nTotal 18 Classes\".format(y_train.max()))\n",
    "\n",
    "# Opportunity num_classes = 18\n",
    "num_classes = 18\n",
    "\n",
    "print(\"F_train shape is \", F_train.shape)\n",
    "print(\"F_test shape is \", F_test.shape)\n",
    "\n",
    "print(\"A_train shape is \", A_train.shape)\n",
    "print(\"A_test shape is \", A_test.shape)\n",
    "\n",
    "\n",
    "print(\"y_train shape is \", y_train.shape)\n",
    "print(\"y_test shape is \", y_test.shape)\n",
    "\n",
    "print(\"===\" * 20)\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- Changing targets shape... (One-Hot Encoder for cross-entropy loss ) \")\n",
    "y_train_1hot = to_categorical(y=y_train, num_classes=num_classes)\n",
    "y_test_1hot = to_categorical(y=y_test, num_classes=num_classes)\n",
    "print(\"Before transforming: y_train shape {0}, y_test shape {2} \\nAfter transforming: y_train_1hot shape {1},y_test_1hot shape {3}\".format(\n",
    "    y_train.shape, y_train_1hot.shape, y_test.shape, y_test_1hot.shape))\n",
    "\n",
    "print(\"--- Successfully\")\n",
    "print(\"===\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Users/chentailin/Dropbox/Dataset/Opportunity/wearable_113_ambient_40_gestures_new.data'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-028d58e98b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# X_train, y_train, X_test, y_test = load_dataset('C:/Users/41762/Dropbox/Dataset/Opportunity/wearable_113_ambient_40_gestures_new.data')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Users/chentailin/Dropbox/Dataset/Opportunity/wearable_113_ambient_40_gestures_new.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;31m# /Users/chentailin/sharedfolder/Human-Activity-Recognition-CodeHub/opportinity-dataset/DeepConvLSTM/data/gestures_wearable_ambient.data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-028d58e98b0c>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Users/chentailin/Dropbox/Dataset/Opportunity/wearable_113_ambient_40_gestures_new.data'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "SLIDING_WINDOW_LENGTH = 24\n",
    "SLIDING_WINDOW_STEP = 12\n",
    "print(\"--- Opporating sliding window...\")\n",
    "SLIDING_WINDOW_LENGTH = 24\n",
    "SLIDING_WINDOW_STEP = 12\n",
    "def opp_sliding_window():\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "F_train0, A_train0, y_train0 = opp_sliding_window(\n",
    "    F_train, A_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "print(\" ..after sliding window (training):\\n inputs : (wearable sensor data) F_train0{0} , (ambient sensor data) A_train0{1}, targets(label) y_train0 {2}\".format(\n",
    "    F_train0.shape, A_train0.shape, y_train0.shape))\n",
    "\n",
    "F_test0, A_test0, y_test0 = opp_sliding_window(\n",
    "    F_test, A_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "print(\" ..after sliding window (testing):\\n inputs : (wearable sensor data) F_test0{0} , (ambient sensor data) A_test0{1}, targets(label) y_test0 {2}\".format(\n",
    "    F_test0.shape, A_test0.shape, y_test0.shape))\n",
    "\n",
    "print(\"Successfully\")\n",
    "print(\"---\" * 20)\n",
    "\n",
    "\n",
    "print(\"Changing targets shape... (One-Hot Encoder for cross-entropy loss ) \")\n",
    "y_train1 = to_categorical(y=y_train0, num_classes=num_classes)\n",
    "y_test1 = to_categorical(y=y_test0, num_classes=num_classes)\n",
    "print(\"Before transforming: y_train0 shape {0}, y_test0 shape {2} \\nAfter transforming: y_train1 shape {1},y_test1 shape {3}\".format(\n",
    "    y_train0.shape, y_train1.shape, y_test0.shape, y_test1.shape))\n",
    "\n",
    "print(\"Successfully\")\n",
    "\n",
    "print(\"---\" * 20)\n",
    "X_train = np.reshape(\n",
    "    F_train0, (F_train0.shape[0], F_train0.shape[1], F_train0.shape[2], 1))  # reshape末尾增加1维\n",
    "X_valid = np.reshape(\n",
    "    F_test0, (F_test0.shape[0], F_test0.shape[1], F_test0.shape[2], 1))\n",
    "y_train = y_train1\n",
    "y_valid = y_test1\n",
    "\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_valid shape\", X_valid.shape)\n",
    "print('y_train.shape' + str(y_train.shape))\n",
    "print('y_train.shape' + str(y_train.shape))\n",
    "\n",
    "print(\"Reshape the X_train and X_valid for CNN input\")\n",
    "X_train_cnn = np.swapaxes(X_train, 1, 2)  # 就是将第三个维度和第二个维度交换\n",
    "X_valid_cnn = np.swapaxes(X_valid, 1, 2)\n",
    "\n",
    "print(\"X_train_cnn shape\", X_train_cnn.shape)\n",
    "print(\"X_valid_cnn shape\", X_valid_cnn.shape)\n",
    "print('X_train.shape' + str(y_train.shape))\n",
    "print('y_train.shape' + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Now, all the data are prepared to be input into the Deep Model\n",
    "\n",
    "- ` F_train0 ` ------> ` y_train1 ` <------` A_train0 `\n",
    "\n",
    "- ` F_test0 ` ------>   ` y_test1 ` <------` A_test0 `\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we noly choose **F** as the feature and **y** as the label\n",
    "- Let \n",
    "        X_train =F_train0\n",
    "        X_test = F_test0\n",
    "        y_train = y_train1\n",
    "        y_test = y_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9894, 24, 113)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_train0.shape\n",
    "F_test0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (46495, 24, 113, 1)\n",
      "X_valid shape (9894, 24, 113, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(F_train0,(F_train0.shape[0],F_train0.shape[1],F_train0.shape[2],1)) # reshape末尾增加1维\n",
    "X_valid = np.reshape(F_test0,(F_test0.shape[0],F_test0.shape[1],F_test0.shape[2],1))\n",
    "y_train = y_train1\n",
    "y_valid = y_test1\n",
    "\n",
    "\n",
    "print(\"X_train shape\" , X_train.shape)\n",
    "print(\"X_valid shape\" , X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape(46495, 24, 113, 1)\n",
      "Y_train.shape(46495, 18)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape' + str(X_train.shape))\n",
    "print('Y_train.shape' + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_basic(x, W, bias):\n",
    "    \"\"\"\n",
    "    x: input data\n",
    "    W: weights\n",
    "    bias: bias\n",
    "    \"\"\"\n",
    "    conv = tf.nn.conv2d(x,W, strides=[1,1,1,1], padding = 'SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "\n",
    "def maxpool_1x2(x):  # a 2*2 size of the window and 2*2 movement of the window step no overlapping\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 2, 1], strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob):\n",
    "    return tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "def permute(x, permute_list):\n",
    "    return tf.transpose(x, perm=permute_list)\n",
    "\n",
    "\n",
    "def mini_batch(x, y, batch_size):\n",
    "    mini_batches = []\n",
    "    num_bathches = int(np.floor(x.shape[0] / batch_size))\n",
    "    for batch_index in range(num_bathches):\n",
    "        mini_batch_x = x[batch_index *\n",
    "                         batch_size:((batch_index * batch_size) + batch_size), :, :, :]\n",
    "        mini_batch_y = y[batch_index *\n",
    "                         batch_size:((batch_index * batch_size) + batch_size), :]\n",
    "        mini_batch = (mini_batch_x, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def create_placeholders(dim, win_len, n_C0, num_classes):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dim :           scalar, dimension of the input ()\n",
    "    win_len :       scalar, window_size of the input()\n",
    "    n_C0 :          scalar, number of channels of the input(commonly 1,time-series data)\n",
    "    num_classes :   sclar, number of classes (num_classes(18))\n",
    "\n",
    "    Return:\n",
    "    x :  placeholder for the data input, of shape[None, win_length,dim,n_C0] and type = 'float'\n",
    "    y :  placeholder for the input labels, of shape[None, num_classes] and type = 'float'\n",
    "    X_train shape is (46495,24,113,1)-->(num_samples,window_length,feature_dims,num_channels)\n",
    "\n",
    "    Now the x is (46495,113,24,1)\n",
    "    \"\"\"\n",
    "    x = tf.placeholder('float32', shape=[None, dim, win_len, n_C0])\n",
    "    y = tf.placeholder('float32', shape=[None, num_classes])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initialize parameters to be used in this model,  dictionary-like!\n",
    "    This method is to create variables basically in our model\n",
    "    did't actually create the neural_network_model\n",
    "    Process:\n",
    "    Input --> Conv2D(num_feat_map,kernel_size=(1,5),padding=same) --> MaxPooling2D(pool_size=(1,2),解释[1])-->\n",
    "    Dropout-->Conv2D(同上                                        ) --> MaxPooling2D (                     )--> Dropout-->Conv2D_Output\n",
    "          --> LSTM_0 -> LSTM_1 -> Dense/Output(num_classes)\n",
    "\n",
    "    解释[1]: pool_size为(1,2) 则原始数据的structure = (dim,win_length，channel） 经过pool_size之后dim不变，win_length减半，和channel无关（不对channel进行操作）若pool_size=（2，1），则dim减半（floor除 地板除结果）， win_length不变\n",
    "    \"\"\"\n",
    "\n",
    "def convolutional_lstm_neural_network(x):\n",
    "\n",
    "        # 5*5 parameters 1 input and 32 feature maps\n",
    "    cnn_weights = {'W_conv1': tf.Variable(tf.random_normal([1, 5, 1, 16])), 'W_conv2': tf.Variable(tf.random_normal([1, 5, 16, 16]))\n",
    "                   }\n",
    "    # biases just the number of the output\n",
    "    cnn_biases = {'b_conv1': tf.Variable(tf.random_normal([16])),  # 5*5 parameters 1 input and 32 feature maps\n",
    "                  'b_conv2': tf.Variable(tf.random_normal([16]))\n",
    "                  # 'b_fc': tf.Variable(tf.random_normal([1024])),\n",
    "                  # 'out': tf.Variable(tf.random_normal([n_classes])),\n",
    "                  }\n",
    "\n",
    "    lstm_weights = {'weights': tf.Variable(tf.random_normal(\n",
    "        [rnn_size, n_classes]))}  # its a parameters dictionary\n",
    "\n",
    "    lstm_biases = {'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    # We get all the parameters then define the model\n",
    "\n",
    "    \"\"\"\n",
    "    Define the data flow chart\n",
    "    \"\"\"\n",
    "    conv1 = conv2d_basic(x, cnn_weights['W_conv1'], cnn_biases['b_conv1']) \n",
    "    conv1 = relu(conv1)\n",
    "    conv1 = maxpool_1x2(conv1)\n",
    "#     conv1 = dropout(conv1, 0.8)\n",
    "\n",
    "    conv2 = conv2d_basic(conv1, cnn_weights['W_conv2'],cnn_biases['b_conv2'])\n",
    "    conv2 = relu(conv2)\n",
    "    conv2 = maxpool_1x2(conv2)\n",
    "#     conv2 = dropout(conv2, 0.8)\n",
    "\n",
    "    # change the output shape (None, 113, 6, 16) to (None, 6, 113, 16)\n",
    "    reshape_1 = permute(conv2, [0, 2, 1, 3])\n",
    "    reshape_2 = tf.reshape(reshape_1, shape=[-1, 6, 113 * 16])\n",
    "\n",
    "    inputs = tf.unstack(reshape_2, 6, 1)\n",
    "\n",
    "    lstm_cell = rnn.BasicLSTMCell(rnn_size)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "    output = tf.matmul(\n",
    "        outputs[-1], lstm_weights['weights']) + lstm_biases['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_neural_network(X_train, y_train, X_valid, y_valid, learning_rate=0.001, num_epochs=6, batch_size=128, print_cost=True):\n",
    "\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    # initialize the placehodler parameters\n",
    "    (m, dim, win_len, n_C0) = X_train.shape\n",
    "    num_classes = y_train.shape[1]\n",
    "\n",
    "    # Create placeholders for X,y and keep_prob\n",
    "    x = tf.placeholder(dtype=tf.float32,shape=[None, dim, win_len, n_C0])\n",
    "    y = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])\n",
    "#     keep_prob = tf.placeholder(tf.float32)\n",
    "#     x, y = create_placeholders(dim, win_len, n_C0, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    # initialize parameters  \n",
    "    prediction = convolutional_lstm_neural_network(x) # prediction\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=prediction, labels=y)) # loss\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss) \n",
    "    \n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "#     writer = tf.summary.FileWriter('./graphs',tf.get_default_graph())\n",
    "    \n",
    "    \n",
    "    # start the session   \n",
    "    with tf.Session() as sess:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch % 3 == 0: \n",
    "                start_time = time.time()\n",
    "                epoch_loss = 0\n",
    "                minibatches = mini_batch(X_train, y_train, batch_size)\n",
    "                for minibatch in minibatches:\n",
    "                    (minibatch_x, minibatch_y) = minibatch\n",
    "\n",
    "                    _, c = sess.run(\n",
    "                        [optimizer, loss], feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                    epoch_loss += c\n",
    "\n",
    "                print('Epoch', epoch, 'completed out of',\n",
    "                      num_epochs, 'Epoch loss', (epoch_loss/batch_size))\n",
    "                print(\"Time: {:3f}\".format(time.time()-start_time))\n",
    "                print('Epoch Accuracy:', accuracy.eval({x: X_valid, y: y_valid}))\n",
    "                \n",
    "#         print('Accuracy:', accuracy.eval({x: X_valid, y: y_valid}))\n",
    "\n",
    "#         writer.close()\n",
    "        print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "# keep_prob\n",
    "\n",
    "n_classes = 18\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "win_length = 24  # 2. imgaes are 28*28 you need to go into sequence. So 28 chunks of 28 pixels\n",
    "dim = 113\n",
    "\n",
    "rnn_size = 64  # num_hidden_lstm units number\n",
    "\n",
    "train_neural_network(X_train_cnn, y_train, X_valid_cnn, y_valid,num_epochs=num_epochs,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
